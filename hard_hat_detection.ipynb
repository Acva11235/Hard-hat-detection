{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Hat detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forming the directory structure and selecting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient det D0 512x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"hard_hat_detection\"\n",
    "CUSTOM_MODEL_NAME = 'hard_hat_detector_efficientdet_d0_200k' \n",
    "PRETRAINED_MODEL_NAME = 'efficientdet_d0_coco17_tpu-32'\n",
    "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz'\n",
    "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "paths = {\n",
    "    'WORKSPACE_PATH': os.path.join('Tensorflow', 'workspace', PROJECT_NAME),\n",
    "    'SCRIPTS_PATH': os.path.join('Tensorflow','scripts'),\n",
    "    'APIMODEL_PATH': os.path.join('Tensorflow','models'),\n",
    "    'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace', PROJECT_NAME,'annotations'),\n",
    "    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace', PROJECT_NAME, 'images'),\n",
    "    'TRAIN_PATH': os.path.join('Tensorflow', 'workspace', PROJECT_NAME, 'images', 'train'),\n",
    "    'TEST_PATH': os.path.join('Tensorflow', 'workspace', PROJECT_NAME, 'images', 'test'),\n",
    "    'MODEL_PATH': os.path.join('Tensorflow', 'workspace', PROJECT_NAME,'models'),\n",
    "    'PRETRAINED_MODEL_PATH': os.path.join('Tensorflow', 'workspace', PROJECT_NAME,'pre-trained-models'),\n",
    "    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace', PROJECT_NAME,'models',CUSTOM_MODEL_NAME), \n",
    "    'OUTPUT_PATH': os.path.join('Tensorflow', 'workspace', PROJECT_NAME,'models',CUSTOM_MODEL_NAME, 'export'), \n",
    "    'TFJS_PATH':os.path.join('Tensorflow', 'workspace', PROJECT_NAME,'models',CUSTOM_MODEL_NAME, 'tfjsexport'), \n",
    "    'TFLITE_PATH':os.path.join('Tensorflow', 'workspace', PROJECT_NAME,'models',CUSTOM_MODEL_NAME, 'tfliteexport'), \n",
    "    'PROTOC_PATH':os.path.join('Tensorflow','protoc'),\n",
    "    'RAW_DATASET_PATH': os.path.join(\"raw_datasets\", PROJECT_NAME),\n",
    "    'TRAIN_RAW_DATASET_PATH': os.path.join(\"raw_datasets\", PROJECT_NAME, 'train'),\n",
    "    'TEST_RAW_DATASET_PATH': os.path.join(\"raw_datasets\", PROJECT_NAME, 'test'),\n",
    "    'MEDIA_PATH': os.path.join('Tensorflow', 'workspace', PROJECT_NAME, 'images', 'media'),\n",
    "    'VIOLATION_LOGGING_PATH': os.path.join('Tensorflow', 'workspace', PROJECT_NAME, 'images', 'violation_logging')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths.values():\n",
    "    if not os.path.exists(path):\n",
    "        if os.name == 'posix':\n",
    "            !mkdir -p {path}\n",
    "        if os.name == 'nt':\n",
    "            !mkdir {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'PIPELINE_CONFIG':os.path.join('Tensorflow', 'workspace', PROJECT_NAME, 'models',CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
    "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME), \n",
    "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imghdr\n",
    "\n",
    "data_directory = paths[\"RAW_DATASET_PATH\"]\n",
    "valid_extensions = [\"jpg\", \"jpeg\", \"png\", \"bmp\"]\n",
    "\n",
    "for image_class in os.listdir(data_directory):\n",
    "    print(f\"Processing {image_class}...\")\n",
    "    for image in os.listdir(os.path.join(data_directory, image_class)):\n",
    "\n",
    "        img_path = os.path.join(data_directory, image_class, image)\n",
    "        file_name, file_extension = os.path.splitext(img_path)\n",
    "\n",
    "        if(file_extension== \".xml\"):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            extension = imghdr.what(img_path)\n",
    "            img_size = os.path.getsize(img_path)\n",
    "\n",
    "            if img_size < 10000:\n",
    "                print(f\"File too small. Removing {image} from {image_class}\")\n",
    "                os.remove(img_path)\n",
    "\n",
    "            if extension not in valid_extensions:\n",
    "                print(f\"Invalid image type. Removing image {image} from {image_class}\")\n",
    "                os.remove(img_path)\n",
    "\n",
    "        except:\n",
    "            print(f\"Error reading file {image}. Removing from {image_class}.\")\n",
    "            os.remove(img_path)\n",
    "\n",
    "\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "for image_class in os.listdir(data_directory):\n",
    "    print(f\"Remaining image in {image_class} are {len(os.listdir(os.path.join(data_directory, image_class)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "data_directory = \"my_dataset\"\n",
    "\n",
    "img_paths = glob.glob(os.path.join(data_directory,'*/*.*')) # assuming you point to the directory containing the label folders.\n",
    "\n",
    "for image_path in img_paths:\n",
    "\n",
    "    file_name, file_extension = os.path.splitext(image_path)\n",
    "\n",
    "    if(file_extension== \".xml\"):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        img_bytes = tf.io.read_file(image_path)\n",
    "        decoded_img = tf.io.decode_image(img_bytes)\n",
    "    except tf.errors.InvalidArgumentError as e:\n",
    "        print(f\"Found bad path {image_path}...{e}. Removing...\")\n",
    "        os.remove(image_path)\n",
    "\n",
    "print(\"\\n\\nScan Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_files(source_directory, target_directory): \n",
    "\n",
    "    print(f\"Processing {source_directory}\")\n",
    "\n",
    "    import shutil\n",
    "    import os\n",
    "\n",
    "    if not target_directory.endswith('/'):\n",
    "        target_directory += '/'\n",
    "\n",
    "    total_moved = 0\n",
    "    valid_extensions = [\"jpg\", \"jpeg\", \"png\", \"bmp\"]\n",
    "\n",
    "    for path in os.listdir(source_directory):\n",
    "\n",
    "        file_name, file_extension = os.path.splitext(path)\n",
    "        \n",
    "        if file_extension[1:] in valid_extensions:\n",
    "            if(os.path.exists(os.path.join(source_directory, file_name + '.xml'))):\n",
    "\n",
    "                shutil.move(os.path.join(source_directory, file_name + '.xml'), target_directory)\n",
    "                shutil.move(os.path.join(source_directory, file_name + file_extension), target_directory)\n",
    "\n",
    "                print(f\"Moved {file_name} and its annotation.\")\n",
    "                total_moved += 1\n",
    "\n",
    "            else:\n",
    "                print(f\"No annotation found for {file_name}. Discarding...\")\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    remaining_files = [item for item in os.listdir(source_directory) if os.path.isfile(os.path.join(source_directory, item))]\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 20)\n",
    "    print(f\"Successfully moved {total_moved} images and their annotations\")\n",
    "    print(f\"Remaining files: {len(remaining_files)}\")\n",
    "    print(\"\\n\" + \"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_files(paths[\"TRAIN_RAW_DATASET_PATH\"], paths[\"TRAIN_PATH\"])\n",
    "move_files(paths[\"TEST_RAW_DATASET_PATH\"], paths[\"TEST_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def is_valid_bbox(bndbox, width, height):\n",
    "    xmin = int(bndbox.find('xmin').text)\n",
    "    xmax = int(bndbox.find('xmax').text)\n",
    "    ymin = int(bndbox.find('ymin').text)\n",
    "    ymax = int(bndbox.find('ymax').text)\n",
    "\n",
    "    if xmin < 0 or ymin < 0 or xmax > width or ymax > height or xmin >= xmax or ymin >= ymax:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def remove_invalid_xml_files(xml_file, valid_classes, directory):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    objects = root.findall('object')\n",
    "    size = root.find('size')\n",
    "    width = int(size.find('width').text)\n",
    "    height = int(size.find('height').text)\n",
    "\n",
    "    for obj in objects:\n",
    "        name = obj.find('name')\n",
    "        bndbox = obj.find('bndbox')\n",
    "\n",
    "        if name is not None and name.text not in valid_classes:\n",
    "            root.remove(obj)\n",
    "\n",
    "        if not is_valid_bbox(bndbox, width, height):\n",
    "\n",
    "            os.remove(xml_file)\n",
    "            img_file = os.path.join(directory, root.find('filename').text)\n",
    "\n",
    "            if os.path.exists(img_file):\n",
    "                os.remove(img_file)\n",
    "            else:\n",
    "                print(f\"No corresponding image found for xml - {xml_file}\")\n",
    "                return 2\n",
    "            \n",
    "            return 1\n",
    "\n",
    "    if len(root.findall('object')) == 0:  # No valid objects left\n",
    "        os.remove(xml_file)\n",
    "        img_file = os.path.join(directory, root.find('filename').text)\n",
    "        if os.path.exists(img_file):\n",
    "            os.remove(img_file)\n",
    "        else:\n",
    "            print(f\"No corresponding image found for xml - {xml_file}\")\n",
    "            return 2\n",
    "\n",
    "        return 1\n",
    "\n",
    "    tree.write(xml_file)\n",
    "    return 0\n",
    "\n",
    "def process_directory(directory, valid_classes):\n",
    "\n",
    "    removed_files = 0\n",
    "    images_to_be_removed = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.xml'):\n",
    "            xml_file = os.path.join(directory, filename)\n",
    "\n",
    "            process_xml_code = remove_invalid_xml_files(xml_file, valid_classes, directory)\n",
    "\n",
    "            if process_xml_code == 1:\n",
    "                print(f\"Removed {filename} (and its corresponding image) due to invalid data.\")\n",
    "                removed_files += 1\n",
    "            elif process_xml_code == 2:\n",
    "                print(f\"No corresponding image found for xml - {filename}. Please remove the image\")\n",
    "                images_to_be_removed.append(filename)\n",
    "            else:\n",
    "                print(f\"Processed file: {filename}\")\n",
    "\n",
    "    print(f\"Successfully removed {removed_files} files...\")\n",
    "    \n",
    "    if(len(images_to_be_removed)):\n",
    "        print(\"Remove the following images. (No corresponding xml files exist. ERROR CODE: 2)\")\n",
    "\n",
    "        for i in images_to_be_removed:\n",
    "            print(i)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import xml.etree.ElementTree as ET\n",
    "\n",
    "# def remove_invalid_objects(xml_file, valid_classes):\n",
    "#     tree = ET.parse(xml_file)\n",
    "#     root = tree.getroot()\n",
    "\n",
    "#     objects = root.findall('object')\n",
    "\n",
    "#     for obj in objects:\n",
    "#         name = obj.find('name')\n",
    "#         if name is not None and name.text not in valid_classes:\n",
    "#             root.remove(obj)\n",
    "#             return 1\n",
    "\n",
    "#     tree.write(xml_file)\n",
    "\n",
    "# def process_directory(directory, valid_classes):\n",
    "#     for filename in os.listdir(directory):\n",
    "#         if filename.endswith('.xml'):\n",
    "#             xml_file = os.path.join(directory, filename)\n",
    "#             if(remove_invalid_objects(xml_file, valid_classes)):\n",
    "#                 print(f\"Invalid object type. Removed {filename}\")\n",
    "#             else:\n",
    "#                 print(f\"Processed file: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_classes = [\"helmet\", \"head\"]\n",
    "\n",
    "process_directory(paths[\"TRAIN_PATH\"], valid_classes)\n",
    "process_directory(paths[\"TEST_PATH\"], valid_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making label map for classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [{'name':'helmet', 'id':1}, {'name':'head', 'id':2}]\n",
    "\n",
    "with open(files['LABELMAP'], 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "if os.name =='posix':\n",
    "    !wget {PRETRAINED_MODEL_URL}\n",
    "    !mv {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
    "    !cd {paths['PRETRAINED_MODEL_PATH']} && tar -xvf {PRETRAINED_MODEL_NAME+'.tar.gz'}\n",
    "if os.name == 'nt':\n",
    "    wget.download(PRETRAINED_MODEL_URL)\n",
    "    !move {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
    "    !cd {paths['PRETRAINED_MODEL_PATH']} && tar -xvf {PRETRAINED_MODEL_NAME+'.tar.gz'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'train')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'train.record')} \n",
    "!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'test')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'test.record')} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the training pipeline\n",
    "\n",
    "Make sure to apply the linear learning rate scaling described here: https://chatgpt.com/share/69662d90-f81a-4cd2-b9ff-48a810cf2271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.name =='posix':\n",
    "    !cp {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(paths['CHECKPOINT_PATH'])}\n",
    "if os.name == 'nt':\n",
    "    !copy {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(paths['CHECKPOINT_PATH'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"r\") as f:                                                                                                                                                                                                                     \n",
    "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "    text_format.Merge(proto_str, pipeline_config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract default values from the config\n",
    "default_batch_size = pipeline_config.train_config.batch_size\n",
    "default_learning_rate_base = pipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.learning_rate_base\n",
    "default_warmup_learning_rate = pipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.warmup_learning_rate\n",
    "\n",
    "# New batch size\n",
    "new_batch_size = 2\n",
    "max_checkpoints = 50\n",
    "\n",
    "print(default_learning_rate_base)\n",
    "print(default_batch_size)\n",
    "\n",
    "# Calculate the new learning rates\n",
    "learning_rate_per_sample = default_learning_rate_base / default_batch_size\n",
    "new_learning_rate_base = learning_rate_per_sample * new_batch_size\n",
    "warmup_ratio = default_warmup_learning_rate / default_learning_rate_base\n",
    "new_warmup_learning_rate = new_learning_rate_base * warmup_ratio\n",
    "\n",
    "# Update the pipeline configuration\n",
    "pipeline_config.model.ssd.num_classes = len(labels)\n",
    "pipeline_config.train_config.batch_size = new_batch_size\n",
    "pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0')\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path= files['LABELMAP']\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'train.record')]\n",
    "pipeline_config.eval_input_reader[0].label_map_path = files['LABELMAP']\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'test.record')]\n",
    "\n",
    "# Update learning rates\n",
    "pipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.learning_rate_base = new_learning_rate_base\n",
    "pipeline_config.train_config.optimizer.momentum_optimizer.learning_rate.cosine_decay_learning_rate.warmup_learning_rate = new_warmup_learning_rate\n",
    "\n",
    "# Print updated learning rates for confirmation\n",
    "print(f\"New learning rate base: {new_learning_rate_base}\")\n",
    "print(f\"New warmup learning rate: {new_warmup_learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:                                                                                                                                                                                                                     \n",
    "    f.write(config_text)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'model_main_tf2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"python {} --model_dir={} --pipeline_config_path={} --num_train_steps=200000\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyperclip\n",
    "\n",
    "pyperclip.copy(command)\n",
    "\n",
    "command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"python {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyperclip\n",
    "\n",
    "pyperclip.copy(command)\n",
    "\n",
    "command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be executed after cd into models/custom_model_name/eval (or train)\n",
    "\n",
    "#tensorboard --logdir=.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "PATH_TO_CFG = files[\"PIPELINE_CONFIG\"]\n",
    "PATH_TO_CKPT = paths[\"CHECKPOINT_PATH\"]\n",
    "\n",
    "print('Loading model... ', end='')\n",
    "start_time = time.time()\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-201')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    \"\"\"Detect objects in image.\"\"\"\n",
    "\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "    return detections\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Done! Took {} seconds'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(files[\"LABELMAP\"],\n",
    "                                                                    use_display_name=True)\n",
    "\n",
    "print(category_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "        path: the file path to the image\n",
    "\n",
    "    Returns:\n",
    "        uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "image_path = os.path.join(paths['IMAGE_PATH'], 'test', '000055_jpg.rf.16c9f577c3e94109f34389b632241d6c.jpg')\n",
    "print('Running inference for {}... '.format(image_path), end='')\n",
    "\n",
    "image_np = load_image_into_numpy_array(image_path)\n",
    "\n",
    "# Things to try:\n",
    "# Flip horizontally\n",
    "# image_np = np.fliplr(image_np).copy()\n",
    "\n",
    "# Convert image to grayscale\n",
    "# image_np = np.tile(\n",
    "#     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
    "\n",
    "input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "\n",
    "detections = detect_fn(input_tensor)\n",
    "\n",
    "# All outputs are batches tensors.\n",
    "# Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "# We're only interested in the first num_detections.\n",
    "num_detections = int(detections.pop('num_detections'))\n",
    "detections = {key: value[0, :num_detections].numpy()\n",
    "                for key, value in detections.items()}\n",
    "detections['num_detections'] = num_detections\n",
    "\n",
    "# detection_classes should be ints.\n",
    "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "label_id_offset = 1\n",
    "image_np_with_detections = image_np.copy()\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np_with_detections,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes']+label_id_offset,\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=200,\n",
    "        min_score_thresh=.30,\n",
    "        agnostic_mode=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image_np_with_detections)\n",
    "print('Done')\n",
    "plt.show()\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect from a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from datetime import datetime, timedelta\n",
    "from screeninfo import get_monitors\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime, timedelta\n",
    "from screeninfo import get_monitors\n",
    "\n",
    "# Path to your video file\n",
    "VIDEO_PATH = os.path.join(paths[\"MEDIA_PATH\"], \"hard_hat_test5.mp4\")\n",
    "print(VIDEO_PATH)\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "\n",
    "label_id_offset = 1\n",
    "\n",
    "head_class_id = None\n",
    "for key, value in category_index.items():\n",
    "    if value['name'] == 'head':\n",
    "        head_class_id = key - label_id_offset\n",
    "        break\n",
    "\n",
    "if head_class_id is None:\n",
    "    print(\"Error: 'head' class not found in category_index.\")\n",
    "    exit()\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "last_violation_time = datetime.now() - timedelta(seconds=15)\n",
    "FRAME_SKIP_INTERVAL = 2\n",
    "frame_count = 0\n",
    "\n",
    "# Get screen resolution\n",
    "screen = get_monitors()[0]\n",
    "screen_width, screen_height = screen.width, screen.height\n",
    "\n",
    "# Get the video's original frame rate\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Define the codec and create a VideoWriter object to save the video in MP4 format\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video_path = os.path.join(paths[\"MEDIA_PATH\"], 'output_video_without_hard_hat.mp4')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (screen_width, screen_height))\n",
    "\n",
    "# Read and display video frames\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Reached the end of the video.\")\n",
    "        break\n",
    "\n",
    "    # frame_count += 1\n",
    "    # if frame_count % FRAME_SKIP_INTERVAL != 0:\n",
    "    #     continue\n",
    "\n",
    "    # Resize frame to fit the screen\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    aspect_ratio = frame_width / frame_height\n",
    "    new_width = screen_width\n",
    "    new_height = int(screen_width / aspect_ratio)\n",
    "    if new_height > screen_height:\n",
    "        new_height = screen_height\n",
    "        new_width = int(screen_height * aspect_ratio)\n",
    "    resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "\n",
    "    image_np = np.array(resized_frame)\n",
    "\n",
    "    # Things to try:\n",
    "    # Flip horizontally\n",
    "    # image_np = np.fliplr(image_np).copy()\n",
    "\n",
    "    # Convert image to grayscale\n",
    "    # image_np = np.tile(\n",
    "    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                    for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np_with_detections,\n",
    "            detections['detection_boxes'],\n",
    "            detections['detection_classes']+label_id_offset,\n",
    "            detections['detection_scores'],\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=5, #Lower value is used for faster processing, default is 200\n",
    "            min_score_thresh=0.4,\n",
    "            agnostic_mode=False)\n",
    "\n",
    "    head_detected = False\n",
    "    for i in range(num_detections):\n",
    "        if (detections['detection_scores'][i] > 0.4 and detections['detection_classes'][i] == head_class_id):\n",
    "            head_detected = True\n",
    "            break\n",
    "\n",
    "    # Save the image if 'head' class is detected and 5 seconds have passed since the last violation\n",
    "    current_time = datetime.now()\n",
    "    if head_detected and current_time - last_violation_time > timedelta(seconds=10):\n",
    "        print(f\"At {current_time} and last time {last_violation_time}\")\n",
    "        violation_image_path = os.path.join(paths[\"VIOLATION_LOGGING_PATH\"], f\"violation_{current_time.strftime('%Y-%m-%d_%H:%M:%S')}.jpg\")\n",
    "        cv2.imwrite(violation_image_path, image_np_with_detections)\n",
    "        last_violation_time = current_time\n",
    "        print(f\"Violation detected. Image saved at: {violation_image_path}\\n\")\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Video', image_np_with_detections)\n",
    "\n",
    "    # Write the frame to the output video file\n",
    "    out.write(cv2.resize(image_np_with_detections, (screen_width, screen_height)))\n",
    "\n",
    "    # Exit the loop when 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and writer objects and close all OpenCV windows\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREEZE_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'exporter_main_v2.py ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"python {} --input_type=image_tensor --pipeline_config_path={} --trained_checkpoint_dir={} --output_directory={}\".format(FREEZE_SCRIPT ,files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'], paths['OUTPUT_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to TFJS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs #Not yet available for windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"tensorflowjs_converter --input_format=tf_saved_model --output_node_names='detection_boxes,detection_classes,detection_features,detection_multiclass_scores,detection_scores,num_detections,raw_detection_boxes,raw_detection_scores' --output_format=tfjs_graph_model --signature_name=serving_default {} {}\".format(os.path.join(paths['OUTPUT_PATH'], 'saved_model'), paths['TFJS_PATH'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFLITE_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'export_tflite_graph_tf2.py ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"python {} --pipeline_config_path={} --trained_checkpoint_dir={} --output_directory={}\".format(TFLITE_SCRIPT ,files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'], paths['TFLITE_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import object_detection\n",
    "\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROZEN_TFLITE_PATH = os.path.join(paths['TFLITE_PATH'], 'saved_model')\n",
    "TFLITE_MODEL = os.path.join(paths['TFLITE_PATH'], 'saved_model', 'detect.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"tflite_convert \\\n",
    "--saved_model_dir={} \\\n",
    "--output_file={} \\\n",
    "--input_shapes=1,300,300,3 \\\n",
    "--input_arrays=normalized_input_image_tensor \\\n",
    "--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n",
    "--inference_type=FLOAT \\\n",
    "--allow_custom_ops\".format(FROZEN_TFLITE_PATH, TFLITE_MODEL, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
